# 工程笔记

## 项目背景
- 目标：将 LoRA 作为“记忆器”，持续吸收 2025 年 FineWeb 新知识，同时最大限度保留旧知识。
- 运行环境：主要在 Modal（A100/H100）执行训练与评测，本地 RTX 5090 作为补充。
- 约束：代码保持 ASCII，训练管线使用此前构建的 `scripts/modal_train_lora.py`、`src/training.py` 等模块。

## 总体方案
1. **数据准备**：
   - 构建旧知识测试集（≤2023 语料）与新知识测试集（2025 FineWeb 快照），确保题目互斥、涵盖多任务类型。
   - 保留原始模型基线得分，用于后续对比。
2. **训练策略对比**：
   - 设计多种 LoRA 方案：仅新数据、旧新混合、带 replay/正则化的增量训练等。
   - 统一使用 Modal 运行，记录 loss、吞吐、显存占用，输出 Adapter 权重。
3. **评测与分析**：
   - 对每个方案分别在旧/新测试集上评测，统计准确率、F1、困惑度等指标。
   - 汇总“新知识增益 vs 旧知识保持率”并结合资源消耗，挑选最优策略。
   - 建立自动化脚本，生成报告与可视化（loss 曲线、性能对比）。

## 工程任务列表
- [ ] 数据集整理：提取旧知识问答、构建 2025 新知识问答，完成标注与格式统一。
- [ ] Baseline 评测：记录原始 Qwen2.5-7B 在两套测试集上的表现。
- [ ] 训练配置编排：补充 LoRA 方案配置文件，明确超参与数据采样策略。
- [ ] Modal 训练自动化：批量运行各方案，归档日志、Adapter 权重、吞吐统计。
- [ ] 评测脚本开发：在 Modal 上执行评测，生成旧/新知识得分与对比表。
- [ ] 报告与可视化：输出训练曲线、性能雷达/散点图，分析遗忘与增益。
- [ ] 持续改进：根据结果调整采样、正则或 replay 策略，形成复用流程。

## 近期优先级
1. 准备并验证新知识小样本评测集，确保评测脚本跑通。
2. 基于 smoke 配置扩展训练规模，观察吞吐与 loss 收敛趋势。
3. 定义第一个对比实验（新数据 vs 混合数据），在 Modal A100 上执行。

## 参考资料
- LiveBench 最新更新（2025-05-30，新增 Agentic Coding 类别）。
- FineWeb CC-MAIN-2025-05/08/13 快照存储在 `fed-lora-traindatasets` 卷。
- `README.md` 中的 Known Issues & Avoidance 需保持同步更新。
